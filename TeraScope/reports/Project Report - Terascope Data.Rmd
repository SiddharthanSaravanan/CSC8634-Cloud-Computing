---
title: "Analysis of Terapixel Visualization"
author: "Siddharthan Saravanan"
date: "25/01/2021"
output: pdf_document
linkcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
```

# Introduction

Terapixel images offer an intuitive, accessible way to present information sets to stakeholders, allowing viewers to interactively browse big data across multiple scales.The challenge that was addressed is how to deliver the SuperComputer scale resources which are needed to compute a realistic Terapixel image and the data was captured by the Newcastle Urban Observatory. The Terapixel image obtained from the data can be viewed through this link, http://terapixel.wasabi.1024.s3.eu-central-1.wasabisys.com/vtour/index.html.

This scalable architecture was created for cloud based visualization where it is possible to deploy and pay for only as needed. The Terapixel image is created in such a way that, it can be viewed in all sorts of devices irrespective of its computational power. This analysis report is to find out the outliers using different GPU's which can be analysed for the improvement of the performance of the Super Computer in generating the image.

## Insights of Data

The dataset which is taken for the analysis was created from application checkpoint and system metric output from the production of the Terapixel image. It particularly consists of three datasets which are application-Checkpoints, GPU and Task-x-y. Each dataset has different aspects which are used for the analysis to improve the performance of generating the Terapixel image. The **Hostname** and **TaskId** are the two variables which plays an important role that interconnects between these three datasets.

*Application-Checkpoints dataset:*

* The Application-Checkpoints dataset consists of six variables that are Timestamp - the time recorded based on each task that has been occured, Hostname - Hostname of the virtual machine auto-assigned by the Azure batch system, EventName - Name of each event occuring while rendering the application, EventType - Type of each event names, jobId - ID of the Azure batch job and TaskId - ID of the Azure batch task.

* There are totally five different EventNames which includes TotalRender - the entire task, Render - the image tile is being rendered, Saving Config - Measure of a configuration, Tiling - Post processing of the rendered tile is taking place and Uploading - The output from post processing is uploaded to Azure storage.

* Each TaskId is linked with the EventNames and each TaskId consists of all the five EventNames.

*GPU dataset:*

* The GPU dataset consists of eight variables that are Timestamp - the time recorded based on the GPU utilization of each hosts, Hostname - Hostname of the virtual machine auto-assigned by the Azure batch system, GPU Serial - The serial number of the physical GPU card, gpuUUID - The unique system id assigned by the Azure system to the GPU unit, powerDrawWatt - Power draw of the GPU in watts, gpuTempC - Temperature of the GPU utilized in Celsius, gpuUtilPerc - Percent utilisation of the GPU Core(s), gpuMemUtilPerc - Percent utilisation of the GPU memory.

* The GPU variables such as power drawn, Temperature, GPY utilization and GPU memory utilization are observed for every two second once each task takes place for rendering the Terapixel image.

*Task-x-y dataset:*

* The Task-x-y dataset consists of five variables that are TaskId - ID of the Azure batch task, jobId - ID of the Azure batch job, x - X co-ordinate of the image tile being rendered, y - Y co-ordinate of the image tile being rendered and level - The visualisation created is a zoomable "google maps style" map.

* In total the image was rendered with the creation of 12 levels of zoom, while for analysing purpose the dataset consists of only 3 levels that are levels 4,8 and 12 where 12 being the zoomed right in level.

* The X and Y variables given represent the co-ordinates of the image being rendered which corresponds to the city of Newcastle Upon Tyne.


```{r include=FALSE}
#Load the project
library('ProjectTemplate')
load.project()
#Execute the analysis files
source("src/Analysis_1.R")
source("src/Analysis_2.R")
source("src/Analysis_3.R")
```

# Analysis

Before proceeding with the analysis, the data is inspected to verify whether the structure of the data is good for analysis and whether the data has duplicate observations in some of the responses. If the data contains duplicate observations then these should be removed as part of the data wrangling process. 

* From the given source of input it is stated that totally there are 65,793 tasks which are being rendered for getting the Terapixel image. 

* Whereas, we could find the total observations in the Application-checkpoints dataset consists of 6,60,400, as each task processes five EventNames each and these EventNames are processed based on exactly two EventTypes, the total Observations should be 6,57,930. 

* This clearly states there might be duplicates in the dataset and these duplicates are removed in both the Application-Checkpoints dataset and GPU dataset for further analysis to take place.

## Data Pre-processing

* The analysis is technical carried out mainly based on the data handling that has been done during the pre-processing process.

* After removing the duplicates from the dataset, all the three datasets are combined respectively into one Master dataset based on the **TaskId**, **Hostname** and **Timestamp** variables. 

* Further to this, the TimeStamp variable is converted completed into seconds which could be easy for the analysis to take place. This process is done by splitting up of the Timestamp variable based on the unknown attributes and converted the Hours, minutes and seconds of the variable to Seconds completely.

* While combining the three datasets as a single Master data there might be *NA* values in the variables, since there wont be any match of the Timestamp observed in the GPU dataset with the Checkpoints dataset as GPU data is observed and noted at every two seconds as the GPU node was being Utilized.

* Hence these are handled by using the Last occurance function which replaces the *NA* value of a variable with the last occured value of that particular variable.

* To calculate the Time taken for each TaskId to complete a particular EventName, a new function Pivot Wider is used which converts the rows to columns based on the given criteria and thus helps in identifying the time taken for the process to complete.

* Looking at the obtained dataset, it is evident that if the GPU utilisation is less than 10 then that particular GPU is not in use and hence, for the graphical analysis new columns have been added which consists of Total Power consumed, Average utilization Temparature, Average GPU utilization and Average GPU memory utilization for each TaskId and it was grouped together.

* After these pre-processing steps, the final Master data is obtained which consists of 2201382 observations with 23 variables.

```{r, echo=FALSE}
head(master_tera_data)
```

* The above displayed dataset is the first few observations of the final Master dataset.

## Graphical Analysis

The graphical analysis is made with Heat maps, line graphs, points, bar plots and box plots which helps to visualize and interpret the data. The **ggplot2** library is used for this graphical analysis.

**Graphical representation 1 :**

```{r, echo=FALSE, figures-side1, fig.show="hold", out.width="50%",fig.height=4, fig.width=7}
par(mar = c(4, 4, .1, .1))
plot_1
plot_2
```

```{r, echo=FALSE, figures-side2, fig.show="hold", out.width="50%",fig.height=4, fig.width=7}
par(mar = c(4, 4, .1, .1))
plot_3
plot_4
```

```{r, echo=FALSE, figures-side3, fig.show="hold", out.width="50%",fig.height=4, fig.width=7}
par(mar = c(4, 4, .1, .1))
plot_5
```

* This graphical analysis is made using the Heat Map to represent time taken for each event to complete its process.

* The regions that are lightly shaded in each graph represents that the time taken for completing process is high and the dark shaded regions represent that the time taken to complete the process is low. 

* It is clearly visible that the EventName - *Render* is taking more time for completing the process and hence while pointing along the heatmap it displayed the Newcastle Town map as all the GPU's are taking time to compute the rendering process. 

* Further to this, the time difference label shown near each graph explains that the other EventNames such as *Saving Config*, *Uploading* and *Tiling* have less time consumption than the *Render* task. 

* The first plot represents the overall time taken for all the process to complete.

**Graphical representation 2 :**

```{r, BoxPlot, echo=FALSE,fig.height=4, fig.width=7}
boxplot_1
```

* The above analysis is made using the Box plot to represent the outliers of each EventNames based on the time difference, when the particular event has completed its process.

* This plot clearly justifies the findings of the first graphical representation that other than the *Render* Event all the other Events takes less time for completing the process.

* In this plot, we could see that there are some outliers for the *Uploading* Event, where the time taken is little higher while comparing to other two events which implies that when there is an increase in time taken by *Render* event there will be significant increase in the *Uploading* event.

**Graphical representation 3 :**

```{r, echo=FALSE, figures-side4, fig.show="hold", out.width="50%",fig.height=4, fig.width=7}
par(mar = c(4, 4, .1, .1))
plot_6
plot_7
```

```{r, echo=FALSE, figures-side5, fig.show="hold", out.width="50%",fig.height=4, fig.width=7}
par(mar = c(4, 4, .1, .1))
plot_8
plot_9
```

* The above generated plots represents the GPU performance analysis which is made using the Heat Map.

* Since the Heat Map is generated based on the x and y co-ordinates of the Terapixel image, we could see that first plot in this representation shows that almost all the regions consume higher power for completing the rendering process.

* The temperature of the GPU won't fall to the bottom even though when there are no GPU's running at that moment and hence all the regions of the image is expected to be running on the average temperature range.

* In this third plot of this representation we could see that the top left of the region is where less number GPU's are being utilized.

* The GPU memory utilization Heat Map represents that some part of the regions have less utilized memory based on the performance.


**Graphical representation 4 :**

```{r, echo=FALSE, figures-side8, fig.show="hold", out.width="50%",fig.height=4, fig.width=7}
par(mar = c(4, 4, .1, .1))
plot_15
plot_16
```

```{r, echo=FALSE, figures-side9, fig.show="hold", out.width="50%",fig.height=4, fig.width=7}
par(mar = c(4, 4, .1, .1))
plot_17
plot_18
```

* The bar graphs represents the most and least GPU variable usages while rendering process takes place.

* Looking at this analysis we could interpret that the total power drawn is very high when there is a high usage of GPU for rendering the image. 

* Similarly some GPU's use less computational power which states that when those GPU's are used for rendering, the performance would significantly increase.

* Further to this, the GPU utilization is more or less in same range for both the most and least used GPU nodes.

* There is not much of a change in the memory utilization also, but if the GPU's are changed with respect to the less memory utilised GPU nodes, this might increase the computational power.

* Similarly if the GPU nodes are changed with those that exhibits less temperature then it could increase the performance while rendering the image.


**Graphical representation 5 :**

```{r, LineGraph, echo=FALSE,fig.height=4, fig.width=7}
plot_14
```

* This Line graph significantly explains that there is an equal amount of spikes in the usage of the power during the execution of each host names.

* The graph is increased when there is a significant usage of power for rendering the image and it is eventually decreased below its mid range when the hosts are not in use.

\newpage
**Graphical representation 6 :**

```{r, echo=FALSE, figures-side6, fig.show="hold", out.width="50%",fig.height=4, fig.width=7}
par(mar = c(4, 4, .1, .1))
plot_10
plot_11
```

```{r, echo=FALSE, figures-side7, fig.show="hold", out.width="50%",fig.height=4, fig.width=7}
par(mar = c(4, 4, .1, .1))
plot_12
plot_13
```

* This graphical representation is based on the outliers of each GPU variables with respect to the average GPU memory utilization while rendering the tasks.

* From these four plots its visible that the top left region of the map takes very less computational power in the rendering process compared to all other regions.

* And the region where the Newcastle stadium is located computes more energy in terms of power drawn, temperature and the GPU utlization, eventhough when there is less usage of GPU memory. 

* This particular analysis is represented by taking out the most and least GPU variable usages while rendering the Terapixel image.


# Business Solution

* The analysis is mainly carried out in order to improve the Performance of the SuperComputer for generating the Terapixel image. 

* From the obtained graphical representations, it is evident that some of the GPU nodes use high power for rendering the Terapixel image and the Temperature of those nodes is equally high.

* To improve the performance of the SuperComputer we could suggest by replacing those high power consuming GPU nodes with respect to the less power consuming GPU nodes.

* Along with this GPU temperature also plays a key role in which by replacing those high temperature consuming GPU nodes with less temperature consuming GPU nodes will significantly improve the performance of the SuperComputer.

# Reproducibility

* Project Template is used to pre-process the data, compute the necessary functions mentioned and then generate the report through the Rmarkdown.

* If there is a new file which is required to generate the report, it should be inserted in the data directory which consists of any of the following three variables (*taskId*, *timestamp* or *hostname*) in order to analyse the performance of the super computer.

* For the purpose of analysis the datasets are combined as a single dataframe and hence if there is any news files which should be reported, it needs to be manually update the pre-process file to combine the data which would be in the munge sub directory.

* The analysis of the report is not completely reproducible as some manual interventions are required to change the variable names, binding up of the datasets as a single dataframe when there appears a new datafile.

* The data file given is of huge size and hence while processing the files are stored in the Cache folder which can be used for multiple runs for reproducing the analysis. 

* Apart from this, the report will be automatically generated when it is made to run from the R markdown report file which is in the Report sub directory.

# Conclusion

After the brief analysis of the observations available, rendering a Terapixel image computes high energy as it is made to be compact for smaller devices also. In order to improve the Performance of the Super Computer for generating such kind of a Terapixel image, it is recommended that replacing the GPU nodes which consumes only less power and less temperature for performing the rendering process could be the ideal solution. 



